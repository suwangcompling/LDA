{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA with raw Python\n",
    "\n",
    "* **Collapsed Gibbs Sampling** (Steyvers & Griffiths, 2007)\n",
    "* **Online Variational Bayes** (Hoffman, Blei & Bach, 2010)\n",
    "\n",
    "NB: a demo with randomly generated toy data, but it clearly shows the code works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "class Indexer(object):\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in xrange(0, len(self))])\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "    def get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "    \n",
    "def normalize(arr):\n",
    "    if type(arr)==list: arr = np.array(arr)\n",
    "    return arr / arr.sum()\n",
    "\n",
    "def safe_log(x, minval=0.0000000001):\n",
    "    return np.log(x.clip(min=minval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{\\textbf{Collapsed Gibbs}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate toy set\n",
    "\n",
    "num_topics = 3\n",
    "topic2words = {0:['cat','dog','horse','pig','lion'],\n",
    "               1:['house','church','room','kitchen','table'],\n",
    "               2:['art','painting','paris','brush','dance']}\n",
    "\n",
    "word_indexer = Indexer()\n",
    "\n",
    "def sample_document(topic_mix, size=100):\n",
    "    doc = []\n",
    "    for i in range(size):\n",
    "        topic = np.random.choice(range(num_topics),p=topic_mix)\n",
    "        rand_topic = np.random.randint(0,num_topics)\n",
    "        word = np.random.choice(topic2words[topic])\n",
    "        w_id = word_indexer.get_index(word)\n",
    "        doc.append((w_id,rand_topic))\n",
    "    return doc\n",
    "\n",
    "num_docs = 100\n",
    "docid2doc = defaultdict(list)\n",
    "cut1, cut2, cut3 = 25, 50, 75 \n",
    "for d in range(num_docs):\n",
    "    if d<cut1:\n",
    "        docid2doc[d] = sample_document([1,0,0])\n",
    "    elif d<cut2:\n",
    "        docid2doc[d] = sample_document([0,1,0])\n",
    "    elif d<cut3:\n",
    "        docid2doc[d] = sample_document([0,0,1])\n",
    "    else:\n",
    "        docid2doc[d] = sample_document([1/3.,1/3.,1/3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W, D, T = len(word_indexer), len(docid2doc), num_topics\n",
    "\n",
    "C_WT, C_DT = np.zeros((W,T)), np.zeros((D,T))\n",
    "\n",
    "for docid in docid2doc.iterkeys():\n",
    "    for w_id,topic in docid2doc[docid]:\n",
    "        C_WT[w_id,topic] += 1\n",
    "        C_DT[docid,topic] += 1\n",
    "\n",
    "beta, alpha = .1, .1\n",
    "beta_vec = np.array([beta]*T); W_beta = W*beta_vec\n",
    "alpha_vec = np.array([alpha]*T); T_alpha = T*alpha_vec\n",
    "\n",
    "def gibbs_sample(w_i, d): # TODO: need to offset by priors!\n",
    "    a = (C_WT[w_i,:] + beta_vec) / (C_WT.sum(axis=0) + W_beta)\n",
    "    b = (C_DT[d,:] + alpha_vec) / (C_DT[d,:].sum() + T_alpha)\n",
    "    c = a * b\n",
    "    p_z = normalize(c)\n",
    "    return np.random.choice(np.arange(T), p=p_z)\n",
    "\n",
    "def document_loglikelihood(d):\n",
    "    a = C_WT / C_WT.sum(axis=0)\n",
    "    b = C_DT[d,:] / C_DT[d,:].sum()\n",
    "    m = a * b\n",
    "    return safe_log(m.sum(axis=1)).sum() # log(W|d)\n",
    "\n",
    "def predict(topic, k=5): # get top k words given topic\n",
    "    return [word_indexer.get_object(w_id) for w_id in np.argsort(C_WT[:,topic])[::-1][:k]]\n",
    "\n",
    "def gibbs_loop(n_iter=10, n_print=1):\n",
    "    for e in range(n_iter):\n",
    "        for docid in docid2doc.iterkeys(): # for each doc\n",
    "            for w_id,topic in docid2doc[docid]: # for each entry\n",
    "                if C_WT[w_id,topic]==0 or C_DT[docid,topic]==0:\n",
    "                    continue\n",
    "                C_WT[w_id,topic] -= 1\n",
    "                C_DT[docid,topic] -= 1\n",
    "                new_topic = gibbs_sample(w_id,docid)\n",
    "                C_WT[w_id,new_topic] += 1\n",
    "                C_DT[docid,new_topic] += 1\n",
    "        if e % n_print == 0:\n",
    "            print repr(document_loglikelihood(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "['art', 'church', 'table', 'painting', 'brush']\n",
      "['lion', 'pig', 'painting', 'art', 'kitchen']\n",
      "['horse', 'room', 'dog', 'pig', 'art']\n",
      "\n",
      "Training\n",
      "-40.639427155306606\n",
      "-41.597639333589107\n",
      "-138.62467082903808\n",
      "-138.72268162918883\n",
      "-138.84041909431863\n",
      "-138.92251789131294\n",
      "-138.95760565243043\n",
      "-139.01725207565858\n",
      "-139.07326292865298\n",
      "-139.08876405935905\n",
      "\n",
      "After\n",
      "['table', 'church', 'house', 'kitchen', 'room']\n",
      "['brush', 'painting', 'art', 'dance', 'paris']\n",
      "['horse', 'pig', 'lion', 'dog', 'cat']\n"
     ]
    }
   ],
   "source": [
    "print \"Before\"\n",
    "for topic in range(T):\n",
    "    print predict(topic)\n",
    "print\n",
    "print \"Training\"\n",
    "gibbs_loop()\n",
    "print\n",
    "print \"After\"\n",
    "for topic in range(T):\n",
    "    print predict(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{\\textbf{Online Variational Bayes}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#docs x vocab-size = (100, 15)\n"
     ]
    }
   ],
   "source": [
    "# Generate toy set (different format from Gibbs)\n",
    "\n",
    "num_topics = 3\n",
    "topic2words = {0:['cat','dog','horse','pig','lion'],\n",
    "               1:['house','church','room','kitchen','table'],\n",
    "               2:['art','painting','paris','brush','dance']}\n",
    "\n",
    "def sample_document(topic_mix, size=100):\n",
    "    doc = np.zeros(len(word_indexer))\n",
    "    for i in range(size):\n",
    "        topic = np.random.choice(range(num_topics),p=topic_mix)\n",
    "        word = np.random.choice(topic2words[topic])\n",
    "        w_id = word_indexer.get_index(word)\n",
    "        doc[w_id] += 1\n",
    "    return doc\n",
    "\n",
    "num_docs = 100\n",
    "docs = []\n",
    "cut1, cut2, cut3 = 25, 50, 75 \n",
    "for d in range(num_docs):\n",
    "    if d<cut1:\n",
    "        docs.append(sample_document([1,0,0]))\n",
    "    elif d<cut2:\n",
    "        docs.append(sample_document([0,1,0]))\n",
    "    elif d<cut3:\n",
    "        docs.append(sample_document([0,0,1]))\n",
    "    else:\n",
    "        docs.append(sample_document([1/3.,1/3.,1/3.]))  \n",
    "docs = np.array(docs)\n",
    "print '#docs x vocab-size = ' + repr(docs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "['pig', 'dance', 'house', 'table', 'lion']\n",
      "['brush', 'lion', 'dance', 'room', 'cat']\n",
      "['cat', 'pig', 'dog', 'art', 'house']\n",
      "\n",
      "Training\n",
      "\n",
      "Epoch 1 (time elapsed: 0.04 secs)\n",
      "   ELBO = -22320.556990169218\n",
      "   Pepl = 9.3190034686573497\n",
      "Epoch 2 (time elapsed: 0.02 secs)\n",
      "   ELBO = -19752.731574365778\n",
      "   Pepl = 7.2085884644568878\n",
      "Epoch 3 (time elapsed: 0.02 secs)\n",
      "   ELBO = -19499.143547714884\n",
      "   Pepl = 7.0280856328126617\n",
      "Epoch 4 (time elapsed: 0.02 secs)\n",
      "   ELBO = -19394.395871931058\n",
      "   Pepl = 6.9548522900106517\n",
      "Epoch 5 (time elapsed: 0.03 secs)\n",
      "   ELBO = -19331.323506937573\n",
      "   Pepl = 6.9111244377147623\n",
      "\n",
      "After\n",
      "['kitchen', 'house', 'table', 'room', 'church']\n",
      "['dance', 'paris', 'brush', 'art', 'painting']\n",
      "['cat', 'lion', 'pig', 'dog', 'horse']\n"
     ]
    }
   ],
   "source": [
    "D = num_docs\n",
    "K = num_topics\n",
    "W = len(word_indexer)\n",
    "\n",
    "alpha = 0.1\n",
    "eta   = 0.01\n",
    "tau0  = 1.0\n",
    "kappa = 0.5\n",
    "\n",
    "def rho(t):\n",
    "    return np.power(tau0 + t, -kappa)\n",
    "\n",
    "alpha_v = np.array([alpha]*K)[:,np.newaxis] # (K,1)\n",
    "eta_m   = np.zeros((K, W))\n",
    "eta_m.fill(eta)\n",
    "\n",
    "l4 = gammaln(K*alpha) - K*gammaln(alpha) + (gammaln(W*eta) - W*gammaln(eta))/D\n",
    "\n",
    "lambda_m = np.random.gamma(100, 1/100, (K, W)) # dist'n over words by topic\n",
    "\n",
    "def predict(topic, k=5):\n",
    "    w_ids = np.argsort(lambda_m[topic]/lambda_m[topic].sum())[::-1][:k]\n",
    "    return [word_indexer.get_object(w_id) for w_id in w_ids]\n",
    "\n",
    "print 'Before'\n",
    "for topic in range(K):\n",
    "    print predict(topic)\n",
    "print\n",
    "print 'Training'\n",
    "print\n",
    "\n",
    "num_epochs    = 5\n",
    "num_iters     = 5\n",
    "epsilon       = 1e-3\n",
    "docidxs       = np.arange(D)\n",
    "n_phi_batch_m = np.zeros((K, W))\n",
    "batch_size    = 10\n",
    "t             = 0 # time step\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    L     = 0 # ELBO\n",
    "    \n",
    "    random.shuffle(docidxs)\n",
    "    for d in docidxs:\n",
    "        n_d   = docs[d] # (W,)\n",
    "        shift = 0.0\n",
    "        # E-step\n",
    "        gamma_d_v = np.random.gamma(100, 1/100, (K, 1))\n",
    "        for i in range(num_iters):\n",
    "            # update phi: (K,W), posteriors over topics\n",
    "            Eq_logtheta_d = psi(gamma_d_v) - psi(gamma_d_v.sum()) # (K,1)\n",
    "            Eq_logbeta    = psi(lambda_m) - psi(lambda_m.sum(axis=1)[:,np.newaxis]) # (K,W)\n",
    "            phi_d_m       = np.exp(Eq_logtheta_d + Eq_logbeta) # (K,W)\n",
    "            phi_d_m       = normalize(phi_d_m, 'l1', axis=0) # posterior over *topics*\n",
    "            # update gamma: (K,1), priors over topics\n",
    "            n_phi_d_m      = phi_d_m * n_d # (K,W)\n",
    "            gamma_d_v_next = alpha_v + n_phi_d_m.sum(axis=1)[:,np.newaxis] # (K,1)\n",
    "            # calculate shift to decide if break\n",
    "            shift = (1/K)*np.abs(gamma_d_v - gamma_d_v_next).sum()\n",
    "            if shift < epsilon:\n",
    "                break\n",
    "            gamma_d_v = gamma_d_v_next\n",
    "        n_phi_batch_m += n_phi_d_m\n",
    "        # M-step\n",
    "        if d % batch_size == 0:\n",
    "            t             += 1 # 1 change every batch\n",
    "            lambda_hat_m  = eta_m + (D/batch_size)*n_phi_batch_m\n",
    "            lambda_m      = (1-rho(t))*lambda_m + rho(t)*lambda_hat_m\n",
    "            n_phi_batch_m = np.zeros((K, W))\n",
    "        # compute ELBO for current doc\n",
    "        l1 = (n_d*(phi_d_m*(Eq_logtheta_d+Eq_logbeta-safe_log(phi_d_m))).sum(axis=0)).sum()\n",
    "        l2 = -gammaln(gamma_d_v.sum())+((alpha_v-gamma_d_v)*Eq_logtheta_d+gammaln(gamma_d_v)).sum()\n",
    "        l3 = (-gammaln(lambda_m.sum(axis=1))+((eta_m-lambda_m)*Eq_logbeta+gammaln(lambda_m)).sum(axis=1)).sum()/D\n",
    "        l = l1 + l2 + l3 + l4\n",
    "        L += l\n",
    "    \n",
    "    print 'Epoch ' + repr(e+1) + ' (time elapsed: ' + \"%.2f secs\" % (time.time()-start) + ')'\n",
    "    print '   ELBO = ' + repr(L)\n",
    "    print '   Pepl = ' + repr(np.exp(-L/docs.sum()))\n",
    "\n",
    "print\n",
    "print 'After'\n",
    "for topic in range(K):\n",
    "    print predict(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
