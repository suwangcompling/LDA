{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-TM with raw Python\n",
    "\n",
    "* **Collapsed Gibbs Sampling** (Steyvers & Griffiths, 2007)\n",
    "* **Online Variational Bayes** (Hoffman, Blei & Bach, 2010)\n",
    "* **Bimodal Topic Model** (Wang, Roller and Erk, 2017)\n",
    "\n",
    "NB: the proof-of-concept demo works with the Quantitative McRae (QMR, Herbelot & Vecchi, 2015) in a bimodal clustering task. There does not seem to be direct extension with Variational methods (as opposed to Gibbs, which does), so Gibbs can even run faster and faster as the topics concentrate. **The clustering looks reasonably even after only 10 Gibbs rounds**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import random\n",
    "import cPickle, dill\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain\n",
    "from operator import add\n",
    "from functools import partial\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.stats.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "class Indexer(object):\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in xrange(0, len(self))])\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "    def get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "    \n",
    "def normalize(arr):\n",
    "    if type(arr)==list: arr = np.array(arr)\n",
    "    return arr / arr.sum()\n",
    "\n",
    "def safe_log(x, minval=0.0000000001):\n",
    "    return np.log(x.clip(min=minval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{\\textbf{Collapsed Gibbs}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#concepts = 500\n",
      "#documents = 500\n",
      "#verb-roles = 29124\n",
      "#features = 220\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "# data_path = '/home/jacobsuwang/Documents/CS TRAINING/VI/DATA/BI-TM/ad.p'\n",
    "data_path = '/home/jacobsuwang/Documents/CS TRAINING/VI/DATA/BI-TM/qmr-freq.p'\n",
    "concepts, cpt2ft, cpt2ftprob, features = cPickle.load(open(data_path, 'r'))\n",
    "# cutoff = int(0.5*len(concepts))\n",
    "# train_concepts, test_concepts = concepts[:cutoff], concepts[cutoff:]\n",
    "\n",
    "# Organize input\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "concepts = set(concepts)\n",
    "cpt_indexer = Indexer()\n",
    "vr_indexer = Indexer()\n",
    "ft_indexer = Indexer()\n",
    "cpt2doc = defaultdict(list)\n",
    "\n",
    "bnc_path = '/home/jacobsuwang/Documents/CS TRAINING/VI/DATA/BI-TM/bnc-triples.txt'\n",
    "\n",
    "with open(bnc_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        if len(line)!=3: continue\n",
    "        cpt,dep,v = line\n",
    "        if cpt in concepts:\n",
    "            cpt_id = cpt_indexer.get_index(cpt)\n",
    "            vr_id = vr_indexer.get_index(v+'-'+dep)\n",
    "            ft = np.random.choice(cpt2ft[cpt], p=normalize(np.array(cpt2ftprob[cpt])))\n",
    "            ft_id = ft_indexer.get_index(ft)\n",
    "            topic = np.random.randint(0, num_topics)\n",
    "            cpt2doc[cpt_id].append((vr_id, ft_id, topic))\n",
    "\n",
    "concepts = set(cpt_indexer.objs_to_ints.keys()) # update concepts\n",
    "            \n",
    "print '#concepts = ' + repr(len(cpt_indexer))\n",
    "print '#documents = ' + repr(len(cpt2doc))\n",
    "print '#verb-roles = ' + repr(len(vr_indexer))\n",
    "print '#features = ' + repr(len(ft_indexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, F, D, T = len(vr_indexer), len(ft_indexer), len(cpt_indexer), num_topics\n",
    "\n",
    "C_WT, C_FT, C_DT = np.zeros((W,T)), np.zeros((F,T)), np.zeros((D,T))\n",
    "\n",
    "for cpt_id in cpt2doc.iterkeys():\n",
    "    for vr_id,ft_id,topic in cpt2doc[cpt_id]:\n",
    "        C_WT[vr_id,topic] += 1\n",
    "        C_FT[ft_id,topic] += 1\n",
    "        C_DT[cpt_id,topic] += 1\n",
    "        \n",
    "beta, gamma, alpha = .1, .1, .1\n",
    "beta_vec = np.array([beta]*T); W_beta = W*beta_vec\n",
    "gamma_vec = np.array([gamma]*T); F_gamma = F*gamma_vec\n",
    "alpha_vec = np.array([alpha]*T); T_alpha = T*alpha_vec\n",
    "\n",
    "def gibbs_sample(w_i, f_i, d): # TODO: need to offset by priors!\n",
    "    a = (C_WT[w_i,:] + beta_vec) / (C_WT.sum(axis=0) + W_beta)\n",
    "    b = (C_FT[f_i,:] + gamma_vec) / (C_FT.sum(axis=0) + F_gamma)\n",
    "    c = (C_DT[d,:] + alpha_vec) / (C_DT[d,:].sum() + T_alpha)\n",
    "    d = a * b * c\n",
    "    p_z = normalize(d)\n",
    "    return np.random.choice(np.arange(T), p=p_z)\n",
    "\n",
    "def document_loglikelihood(d):\n",
    "    a = C_WT / C_WT.sum(axis=0)\n",
    "    b = C_DT[d,:] / C_DT[d,:].sum()\n",
    "    m = a * b\n",
    "    return safe_log(m.sum(axis=1)).sum() # log(W|d)\n",
    "\n",
    "def data_loglikelihood(): # sum_d log(W|d)\n",
    "    return np.array([document_loglikelihood(d) for d in cpt2doc.iterkeys()]).sum()\n",
    "\n",
    "def predict(d, k=5):\n",
    "    a = C_FT / C_FT.sum(axis=0)\n",
    "    b = C_DT[d,:] / C_DT[d,:].sum()\n",
    "    p_f = (a * b).sum(axis=1)\n",
    "    return [ft_indexer.get_object(f_id) for f_id in np.argsort(p_f)[::-1][:k]]\n",
    "\n",
    "def gibbs_loop(n_iter=10, n_print=1):\n",
    "    for e in range(n_iter):\n",
    "        start = time.time()\n",
    "        for cpt_id in cpt2doc.iterkeys(): # for each doc\n",
    "            for vr_id,ft_id,topic in cpt2doc[cpt_id]: # for each entry\n",
    "                if C_WT[vr_id,topic]==0 or C_FT[ft_id,topic]==0 or C_DT[cpt_id,topic]==0:\n",
    "                    continue\n",
    "                C_WT[vr_id,topic] -= 1\n",
    "                C_FT[ft_id,topic] -= 1\n",
    "                C_DT[cpt_id,topic] -= 1\n",
    "                new_topic = gibbs_sample(vr_id,ft_id,cpt_id)\n",
    "                C_WT[vr_id,new_topic] += 1\n",
    "                C_FT[ft_id,new_topic] += 1\n",
    "                C_DT[cpt_id,new_topic] += 1\n",
    "        if e % n_print == 0:\n",
    "            print 'Epoch ' + repr(e+1) + ' time = ' + repr(time.time()-start)\n",
    "            print repr(document_loglikelihood(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before\n",
      "level [u'made_of_paper', u'made_of_wood', u'made_of_metal', u'a_tool', u'is_round']\n",
      "book [u'made_of_paper', u'made_of_wood', u'made_of_metal', u'a_tool', u'is_round']\n",
      "certificate [u'made_of_paper', u'made_of_wood', u'made_of_metal', u'a_tool', u'is_round']\n",
      "card [u'made_of_paper', u'made_of_wood', u'made_of_metal', u'a_tool', u'is_round']\n",
      "telephone [u'made_of_paper', u'made_of_wood', u'made_of_metal', u'a_tool', u'is_round']\n",
      "\n",
      "Training\n",
      "Epoch 1 time = 210.933434009552\n",
      "-346433.28793915332\n",
      "Epoch 2 time = 168.16139888763428\n",
      "-349299.93902262847\n",
      "Epoch 3 time = 106.8126711845398\n",
      "-367112.80546143511\n",
      "Epoch 4 time = 60.34106707572937\n",
      "-401908.83980329288\n",
      "Epoch 5 time = 41.439839124679565\n",
      "-460935.82489537925\n",
      "Epoch 6 time = 33.05083417892456\n",
      "-497566.87269232917\n",
      "Epoch 7 time = 28.408526182174683\n",
      "-526617.91637018719\n",
      "Epoch 8 time = 25.34993600845337\n",
      "-528625.00486221421\n",
      "Epoch 9 time = 23.177672863006592\n",
      "-530670.47874918918\n",
      "Epoch 10 time = 21.459289073944092\n",
      "-532923.07565391448\n",
      "\n",
      "After\n",
      "level [u'a_tool', u'made_of_wood', u'used_for_construction', u'used_for_carpentry', u'is_round']\n",
      "book [u'made_of_paper', u'is_rectangular', u'has_a_handle', u'clothing', u'different_colours']\n",
      "certificate [u'made_of_paper', u'is_rectangular', u'made_of_wood', u'is_electrical', u'has_a_handle']\n",
      "card [u'made_of_paper', u'is_colourful', u'is_smooth', u'is_long', u'made_of_metal']\n",
      "telephone [u'used_for_holding_things', u'furniture', u'has_legs', u'made_of_plastic', u'made_of_paper']\n"
     ]
    }
   ],
   "source": [
    "print \"Before\"\n",
    "for cpt_id in cpt2doc.keys()[:5]:\n",
    "    print cpt_indexer.get_object(cpt_id) + ' ' + repr(predict(cpt_id))\n",
    "print\n",
    "print \"Training\"\n",
    "gibbs_loop()\n",
    "print\n",
    "print \"After\"\n",
    "for cpt_id in cpt2doc.keys()[:5]:\n",
    "    print cpt_indexer.get_object(cpt_id) + ' ' + repr(predict(cpt_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large{\\textbf{Online Variational Bayes}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize as sk_normalize\n",
    "from scipy.special import gammaln\n",
    "from scipy.special import psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#concepts = 500\n",
      "#documents = 500\n",
      "#verb-roles = 29124\n",
      "#features = 220\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "# data_path = '/home/jacobsuwang/Documents/CS TRAINING/VI/DATA/BI-TM/ad.p'\n",
    "data_path = '/home/jacobsuwang/Documents/CS TRAINING/VI/DATA/BI-TM/qmr-freq.p'\n",
    "concepts, cpt2ft, cpt2ftprob, features = cPickle.load(open(data_path, 'r'))\n",
    "# cutoff = int(0.5*len(concepts))\n",
    "# train_concepts, test_concepts = concepts[:cutoff], concepts[cutoff:]\n",
    "\n",
    "# Organize input\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "concepts = set(concepts)\n",
    "cpt_indexer = Indexer()\n",
    "vr_indexer = Indexer()\n",
    "ft_indexer = Indexer()\n",
    "cpt2vrdoc = defaultdict(list)\n",
    "cpt2ftdoc = defaultdict(list)\n",
    "\n",
    "bnc_path = '/home/jacobsuwang/Documents/CS TRAINING/VI/DATA/BI-TM/bnc-triples.txt'\n",
    "\n",
    "with open(bnc_path, 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        if len(line)!=3: continue\n",
    "        cpt,dep,v = line\n",
    "        if cpt in concepts:\n",
    "            cpt_id = cpt_indexer.get_index(cpt)\n",
    "            vr_id = vr_indexer.get_index(v+'-'+dep)\n",
    "            ft = np.random.choice(cpt2ft[cpt], p=normalize(np.array(cpt2ftprob[cpt])))\n",
    "            ft_id = ft_indexer.get_index(ft)\n",
    "            cpt2vrdoc[cpt_id].append(vr_id)\n",
    "            cpt2ftdoc[cpt_id].append(ft_id)\n",
    "\n",
    "concepts = set(cpt_indexer.objs_to_ints.keys()) # update concepts\n",
    "            \n",
    "print '#concepts = ' + repr(len(cpt_indexer))\n",
    "print '#documents = ' + repr(len(cpt2vrdoc)); assert len(cpt2vrdoc)==len(cpt2ftdoc)\n",
    "print '#verb-roles = ' + repr(len(vr_indexer))\n",
    "print '#features = ' + repr(len(ft_indexer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (time elapsed: 268.46 secs)\n",
      "Epoch 2 (time elapsed: 254.09 secs)\n",
      "Epoch 3 (time elapsed: 298.25 secs)\n",
      "Epoch 4 (time elapsed: 254.21 secs)\n",
      "Epoch 5 (time elapsed: 261.84 secs)\n"
     ]
    }
   ],
   "source": [
    "D = len(cpt_indexer)\n",
    "K = num_topics\n",
    "W = len(vr_indexer)\n",
    "F = len(ft_indexer)\n",
    "\n",
    "alpha = 0.1\n",
    "eta   = 0.01\n",
    "zeta  = 0.01\n",
    "tau0  = 1.0\n",
    "kappa = 0.5\n",
    "\n",
    "def rho(t):\n",
    "    return np.power(tau0 + t, -kappa)\n",
    "\n",
    "alpha_v = np.array([alpha]*K)[:,np.newaxis] # (K,1)\n",
    "eta_m   = np.zeros((K, W)); eta_m.fill(eta)\n",
    "zeta_m  = np.zeros((K, F)); zeta_m.fill(eta)\n",
    "\n",
    "# todo: term4 (NB: haven't worked out the ELBO)\n",
    "#       if this works, monitor MAP score of property predictions\n",
    "#       instead!\n",
    "\n",
    "lambda_m = np.random.gamma(100, 1/100, (K, W))\n",
    "xi_m     = np.random.gamma(100, 1/100, (K, F))\n",
    "\n",
    "# todo: predict p(f|d) (require repackaging, because ...)\n",
    "def predict(topic, k=5):\n",
    "    w_ids = np.argsort(lambda_m[topic]/lambda_m[topic].sum())[::-1][:k]\n",
    "    f_ids = np.argsort(xi_m[topic]/xi_m[topic].sum())[::-1][:k]\n",
    "    vrs   = [vr_indexer.get_object(w_id) for w_id in w_ids]\n",
    "    fts   = [ft_indexer.get_object(f_id) for f_id in f_ids]\n",
    "    return vrs, fts\n",
    "# todo: print before results\n",
    "\n",
    "num_epochs    = 5\n",
    "num_iters     = 5\n",
    "epsilon       = 1e-3\n",
    "docidxs       = np.arange(D)\n",
    "# todo: batch matrix (may be not able to do batch)\n",
    "batch_size    = 10\n",
    "t             = 0 # time step\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for _ in range(int(D/batch_size)):\n",
    "        batch_docidxs = np.random.choice(docidxs,batch_size,replace=False)\n",
    "        phi_w_m = np.zeros((K, W)) # batch cache of phi info\n",
    "        phi_f_m = np.zeros((K, F))    \n",
    "        for d in batch_docidxs:\n",
    "            wf_pairs = zip(cpt2vrdoc[d],cpt2ftdoc[d])\n",
    "            N_d      = len(wf_pairs)\n",
    "            shift    = 0.0\n",
    "            gamma_d_v = np.random.gamma(100, 1/100, (K, 1))\n",
    "            for _ in range(num_iters):\n",
    "                phi_d_m = np.zeros((K,N_d)) # (K,N_d)\n",
    "                for i,(w,f) in enumerate(wf_pairs): # d,w,f are int ids\n",
    "                    Eq_logtheta_d = psi(gamma_d_v) - psi(gamma_d_v.sum()) # (K,1)\n",
    "                    Eq_logbeta_w  = psi(lambda_m[:,w][:,np.newaxis]) \\\n",
    "                                    - psi(lambda_m.sum(axis=1)[:,np.newaxis]) # (K,1)\n",
    "                    Eq_logdelta_f = psi(xi_m[:,f][:,np.newaxis]) \\\n",
    "                                    - psi(xi_m.sum(axis=1)[:,np.newaxis]) # (K,1)\n",
    "                    phi_d_m[:,i]  = np.exp(Eq_logtheta_d+Eq_logbeta_w+Eq_logdelta_f).T # update the ith column\n",
    "                    phi_i_v = sk_normalize([phi_d_m[:,i]], 'l1')[0] # (K,), phi info for the ith entry\n",
    "                    phi_d_m[:,i] = phi_i_v # replace the original with normalized\n",
    "                    phi_w_m[:,w] += phi_i_v # update for corresponding word\n",
    "                    phi_f_m[:,f] += phi_i_v # update for corresponding feature\n",
    "                # todo: select cols to update \n",
    "                gamma_d_v_next = alpha_v + phi_d_m.sum(axis=1)[:,np.newaxis] # (K,1)\n",
    "                shift = (1/K)*np.abs(gamma_d_v - gamma_d_v_next).sum()\n",
    "                if shift < epsilon:\n",
    "                    break\n",
    "                gamma_d_v = gamma_d_v_next\n",
    "                \n",
    "        t            += 1\n",
    "        lambda_hat_m = eta_m + (D/batch_size)*phi_w_m\n",
    "        lambda_m     = (1-rho(t))*lambda_m + rho(t)*lambda_hat_m\n",
    "        xi_hat_m     = zeta_m + (D/batch_size)*phi_f_m\n",
    "        xi_m         = (1-rho(t))*xi_m + rho(t)*xi_hat_m\n",
    "    \n",
    "    print 'Epoch ' + repr(e+1) + ' (time elapsed: ' + \"%.2f secs\" % (time.time()-start) + ')'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "contexts: ['high-amod', 'low-amod', 'sea-compound', 'local-amod', 'different-amod']\n",
      "properties: [u'a_tool', u'made_of_wood', u'used_for_construction', u'used_for_carpentry', u'is_hard']\n",
      "\n",
      "Topic 1\n",
      "contexts: ['little-amod', 'old-amod', 'young-amod', 'large-amod', 'come-nsubj']\n",
      "properties: [u'an_animal', u'a_mammal', u'has_legs', u'has_4_legs', u'has_a_tail']\n",
      "\n",
      "Topic 2\n",
      "contexts: ['hit-dobj', 'come-nsubj', 'use-dobj', 'new-amod', 'catch-dobj']\n",
      "properties: [u'is_fun', u'used_for_transportation', u'a_tool', u'made_of_wood', u'has_wheels']\n",
      "\n",
      "Topic 3\n",
      "contexts: ['dressing-compound', 'stone-compound', 'new-amod', 'big-amod', 'white-amod']\n",
      "properties: [u'found_in_houses', u'used_for_protection', u'made_of_wood', u'has_buttons', u'is_electrical']\n",
      "\n",
      "Topic 4\n",
      "contexts: ['read-dobj', 'write-dobj', 'new-amod', 'seat-compound', 'frying-compound']\n",
      "properties: [u'made_of_paper', u'has_a_lid', u'used_for_holding_things', u'is_round', u'is_transparent']\n",
      "\n",
      "Topic 5\n",
      "contexts: ['play-dobj', 'diamond-compound', 'use-dobj', 'letter-compound', 'barn-compound']\n",
      "properties: [u'made_of_metal', u'is_round', u'made_of_paper', u'made_of_wood', u'is_expensive']\n",
      "\n",
      "Topic 6\n",
      "contexts: ['new-amod', 'old-amod', 'build-dobj', 'little-amod', 'small-amod']\n",
      "properties: [u'is_large', u'made_of_wood', u'is_small', u'a_building', u'a_house']\n",
      "\n",
      "Topic 7\n",
      "contexts: ['use-dobj', 'carry-dobj', 'hold-dobj', 'cross-dobj', 'wooden-amod']\n",
      "properties: [u'made_of_wood', u'is_long', u'made_of_metal', u'a_tool', u'has_a_handle']\n",
      "\n",
      "Topic 8\n",
      "contexts: ['wear-dobj', 'white-amod', 'black-amod', 'red-amod', 'long-amod']\n",
      "properties: [u'clothing', u'is_hard', u'different_colours', u'is_long', u'worn_for_protection']\n",
      "\n",
      "Topic 9\n",
      "contexts: ['wedding-compound', 'light-dobj', 'gold-compound', 'smoke-dobj', 'little-amod']\n",
      "properties: [u'is_round', u'is_expensive', u'has_a_lid', u'is_small', u'is_edible']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in range(K):\n",
    "    vrs,fts = predict(topic)\n",
    "    print 'Topic ' + repr(topic)\n",
    "    print 'contexts: ' + repr(vrs)\n",
    "    print 'properties: ' + repr(fts)\n",
    "    print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
