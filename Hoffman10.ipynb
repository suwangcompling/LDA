{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoffman et al. (2010)\n",
    "\n",
    "* **Batch Variational Bayes**\n",
    "* **Online Variational Bayes (Stochastic)**\n",
    "* **Online Variational Bayes (Mini-batch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from string import punctuation as punc\n",
    "from spacy.en import STOPWORDS\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2count = Counter([word.lower() for word in brown.words()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def strip(word):\n",
    "    word = word.lower()\n",
    "    return False if (word in STOPWORDS or word in punc or word2count[word]>1000 or word2count[word]<10 or (not word.isalpha())) else word\n",
    "def strip_doc(doc): # doc: a list of sullied words.\n",
    "    clean_doc = []\n",
    "    for word in doc:\n",
    "        clean_word = strip(word)\n",
    "        if clean_word:\n",
    "            clean_doc.append(clean_word)\n",
    "    return clean_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 11554\n",
      "# files: 500\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(word for word in brown.words() if strip(word)))\n",
    "fileids = brown.fileids()\n",
    "i2w = {i:w for i,w in enumerate(vocab)}\n",
    "w2i = {w:i for i,w in i2w.iteritems()}\n",
    "print 'vocab size:', len(vocab)\n",
    "print '# files:', len(fileids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "brown_docs = [strip_doc(brown.words(fileids=fileid)) for fileid in fileids] \n",
    "print len(brown_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "from scipy.special import digamma\n",
    "from scipy.special import gammaln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Variational Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = {}\n",
    "num_tokens = 0\n",
    "for d,doc in enumerate(brown_docs):\n",
    "    w2c = Counter(doc)\n",
    "    N[d] = np.array([w2c[w] for w in vocab])\n",
    "    num_tokens += N[d].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 10 # number of topics\n",
    "W = len(vocab)\n",
    "\n",
    "alpha = 0.1\n",
    "eta   = 0.01\n",
    "\n",
    "num_docs   = len(N)\n",
    "num_epochs = 5\n",
    "num_iters  = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "... current shift = 0.572711781583\n",
      "... current shift = 0.414873927668\n",
      "... current shift = 0.74968012405\n",
      "... current shift = 0.767346051529\n",
      "... current shift = 0.561619147587\n",
      "... current shift = 0.676928498971\n",
      "... current shift = 0.675965892509\n",
      "... current shift = 0.563462110808\n",
      "... current shift = 0.708754030714\n",
      "... current shift = 0.537567183138\n",
      "@epoch 1\n",
      "   ELBO = -3277734.55452\n",
      "   Perplexity = 15217.8956769\n",
      "(time elapsed: 51.6999080181)\n",
      "Epoch 2\n",
      "... current shift = 1.35892358408\n",
      "... current shift = 1.05407248658\n",
      "... current shift = 1.95318664544\n",
      "... current shift = 1.36361188849\n",
      "... current shift = 0.921860171214\n",
      "... current shift = 1.1463568151\n",
      "... current shift = 2.03817040101\n",
      "... current shift = 1.62714043656\n",
      "... current shift = 1.18262902962\n",
      "... current shift = 1.18801256657\n",
      "@epoch 2\n",
      "   ELBO = -2665063.41491\n",
      "   Perplexity = 2515.31692128\n",
      "(time elapsed: 47.8911709785)\n",
      "Epoch 3\n",
      "... current shift = 2.81271704324\n",
      "... current shift = 2.2642581148\n",
      "... current shift = 3.67728399879\n",
      "... current shift = 2.40722029738\n",
      "... current shift = 2.06417994803\n",
      "... current shift = 2.32618523121\n",
      "... current shift = 6.18786106762\n",
      "... current shift = 3.15054189399\n",
      "... current shift = 2.99617069585\n",
      "... current shift = 3.46857143265\n",
      "@epoch 3\n",
      "   ELBO = -2643929.30469\n",
      "   Perplexity = 2363.88209685\n",
      "(time elapsed: 46.8994560242)\n",
      "Epoch 4\n",
      "... current shift = 3.84966879577\n",
      "... current shift = 3.61038175286\n",
      "... current shift = 4.20440002751\n",
      "... current shift = 2.93745078947\n",
      "... current shift = 3.5055779857\n",
      "... current shift = 3.63058913726\n",
      "... current shift = 2.4580739099\n",
      "... current shift = 2.90566604985\n",
      "... current shift = 3.05393347266\n",
      "... current shift = 2.91456728762\n",
      "@epoch 4\n",
      "   ELBO = -2573086.63417\n",
      "   Perplexity = 1919.69093294\n",
      "(time elapsed: 47.0947020054)\n",
      "Epoch 5\n",
      "... current shift = 3.38256883343\n",
      "... current shift = 3.62317178253\n",
      "... current shift = 2.86060232713\n",
      "... current shift = 2.06912738574\n",
      "... current shift = 2.77627149699\n",
      "... current shift = 3.46760168489\n",
      "... current shift = 0.716514108058\n",
      "... current shift = 2.17213219154\n",
      "... current shift = 2.48202314522\n",
      "... current shift = 3.34690890875\n",
      "@epoch 5\n",
      "   ELBO = -2492330.52108\n",
      "   Perplexity = 1514.21438619\n",
      "(time elapsed: 47.5359811783)\n",
      "CPU times: user 4min 1s, sys: 7 ms, total: 4min 1s\n",
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Alpha = np.ones(K); Alpha.fill(alpha)\n",
    "Eta   = np.ones((K, W)); Eta.fill(eta)\n",
    "\n",
    "term4 = gammaln(K*alpha) - K*gammaln(alpha) + (gammaln(W*eta) - W*gammaln(eta))/num_docs\n",
    "\n",
    "Lambda = np.random.gamma(100., 1./100, (K, W))\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    print 'Epoch', e+1 \n",
    "    L = 0\n",
    "    \n",
    "    t = time.time()\n",
    "    \n",
    "    n_Phi = np.zeros((K, W))\n",
    "    for d in range(num_docs):\n",
    "        \n",
    "        n_d = N[d]\n",
    "\n",
    "        shift = np.inf\n",
    "        epsilon = 1e-3\n",
    "\n",
    "        Gamma_d = np.random.gamma(100., 1./100, K)                  # K\n",
    "        for _ in range(num_iters):\n",
    "\n",
    "            sum_Gamma_d = Gamma_d.sum()                             # 1\n",
    "            sum_Lambda  = Lambda.sum(axis=1)                        # K x 1\n",
    "            \n",
    "            Eq_logtheta_d = digamma(Gamma_d) - digamma(sum_Gamma_d)               # K - 1 = K\n",
    "            Eq_logbeta    = digamma(Lambda) - digamma(sum_Lambda[:,np.newaxis])   # K x W - K x 1 = K x W\n",
    "            \n",
    "            Eq_logtheta_add_Eq_logbeta = Eq_logtheta_d[:,np.newaxis] + Eq_logbeta # for likelihood computation later.\n",
    "\n",
    "            Phi_d = np.exp(Eq_logtheta_add_Eq_logbeta)  # K x W\n",
    "            Phi_d = normalize(Phi_d, norm='l1', axis=0) # each word relates to a mult. over topics\n",
    "            n_Phi_d = Phi_d * n_d # K x W\n",
    "\n",
    "            Gamma_delta = Alpha + n_Phi_d.sum(axis=1) # K\n",
    "\n",
    "            shift = (1/K)*abs(Gamma_d - Gamma_delta).sum()\n",
    "            if shift < epsilon:\n",
    "                break\n",
    "                \n",
    "            Gamma_d = Gamma_delta\n",
    "        \n",
    "        if d%50==0:\n",
    "            print '... current shift =', shift\n",
    "\n",
    "        n_Phi += n_Phi_d\n",
    "        \n",
    "        # COMPUTE LIKELIHOOD (at some point)\n",
    "        term1 = (n_d * (Phi_d * (Eq_logtheta_add_Eq_logbeta - np.log(Phi_d))).sum(axis=0)).sum()\n",
    "        term2 = -gammaln(sum_Gamma_d) + ((Alpha-Gamma_d)*Eq_logtheta_d + gammaln(Gamma_d)).sum()\n",
    "        term3 = (-gammaln(sum_Lambda) + ((Eta-Lambda)*Eq_logbeta + gammaln(Lambda)).sum(axis=1)).sum() / num_docs \n",
    "        l = term1 + term2 + term3 + term4 \n",
    "        L += l\n",
    "\n",
    "    Lambda = Eta + n_Phi # K x W\n",
    "\n",
    "    print \"@epoch\", e+1\n",
    "    print \"   ELBO =\", L\n",
    "    print \"   Perplexity =\", np.exp(-(L/num_tokens))\n",
    "\n",
    "    print '(time elapsed:', str(time.time()-t)+')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words(topic, topk=20):\n",
    "    word_dist = Lambda[topic] / Lambda[topic].sum()\n",
    "    print map(lambda i:i2w[i], np.argsort(word_dist)[::-1][:topk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "[u'world', u'life', u'social', u'great', u'human', u'way', u'people', u'group', u'experience', u'men', u'years', u'sense', u'fact', u'society', u'work', u'art', u'power', u'war', u'good', u'feed']\n",
      "Topic: 1\n",
      "[u'college', u'people', u'school', u'years', u'students', u'course', u'stress', u'student', u'class', u'service', u'medical', u'year', u'stations', u'high', u'world', u'national', u'work', u'fact', u'good', u'president']\n",
      "Topic: 2\n",
      "[u'church', u'use', u'school', u'area', u'clay', u'economic', u'social', u'god', u'costs', u'people', u'life', u'index', u'churches', u'world', u'members', u'program', u'action', u'cost', u'electronic', u'place']\n",
      "Topic: 3\n",
      "[u'state', u'law', u'tax', u'year', u'policy', u'states', u'number', u'program', u'development', u'public', u'years', u'local', u'property', u'government', u'problem', u'fiscal', u'business', u'world', u'general', u'economic']\n",
      "Topic: 4\n",
      "[u'states', u'united', u'general', u'use', u'government', u'shall', u'business', u'cost', u'small', u'year', u'industry', u'act', u'company', u'state', u'federal', u'increase', u'equipment', u'radiation', u'rate', u'sales']\n",
      "Topic: 5\n",
      "[u'know', u'good', u'long', u'way', u'little', u'thought', u'old', u'got', u'years', u'day', u'looked', u'came', u'eyes', u'form', u'place', u'asked', u'head', u'people', u'face', u'love']\n",
      "Topic: 6\n",
      "[u'house', u'came', u'way', u'right', u'come', u'little', u'thought', u'left', u'door', u'away', u'hand', u'old', u'eyes', u'good', u'went', u'head', u'knew', u'room', u'long', u'got']\n",
      "Topic: 7\n",
      "[u'little', u'know', u'went', u'came', u'old', u'home', u'day', u'long', u'way', u'men', u'told', u'got', u'good', u'come', u'right', u'knew', u'people', u'think', u'look', u'looked']\n",
      "Topic: 8\n",
      "[u'state', u'president', u'war', u'united', u'men', u'states', u'people', u'committee', u'military', u'house', u'party', u'government', u'year', u'nuclear', u'national', u'city', u'years', u'week', u'country', u'west']\n",
      "Topic: 9\n",
      "[u'data', u'reaction', u'age', u'possible', u'work', u'volume', u'study', u'given', u'state', u'present', u'normal', u'number', u'type', u'pressure', u'obtained', u'large', u'years', u'small', u'results', u'surface']\n"
     ]
    }
   ],
   "source": [
    "for topic in range(K):\n",
    "    print 'Topic:', topic\n",
    "    top_words(topic, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Variational Bayes (Stochastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = {}\n",
    "num_tokens = 0\n",
    "for d,doc in enumerate(brown_docs):\n",
    "    w2c = Counter(doc)\n",
    "    N[d] = np.array([w2c[w] for w in vocab])\n",
    "    num_tokens += N[d].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 10 # number of topics\n",
    "W = len(vocab)\n",
    "\n",
    "alpha = 0.1\n",
    "eta   = 0.01\n",
    "tau0  = 1.0\n",
    "kappa = 0.5\n",
    "rho   = lambda t: np.power(tau0 + t, -kappa)\n",
    "\n",
    "num_docs   = len(N)\n",
    "num_epochs = 5 # Doesn't have too. If too many docs, 1 pass.\n",
    "num_iters  = 10\n",
    "\n",
    "doc_ids = range(num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "... current shift = 0.362500538985\n",
      "... current shift = 0.127364215178\n",
      "... current shift = 0.000908639833014\n",
      "... current shift = 0.000838237428591\n",
      "... current shift = 0.000543284603562\n",
      "... current shift = 0.808451832755\n",
      "... current shift = 0.172544761524\n",
      "... current shift = 0.237097432929\n",
      "... current shift = 0.0166639313605\n",
      "... current shift = 0.0209491011069\n",
      "@epoch 1\n",
      "   ELBO = -4554554.33239\n",
      "   Perplexity = 647982.274947\n",
      "(time elapsed: 753.123986006)\n",
      "Epoch: 2\n",
      "... current shift = 0.00104451531615\n",
      "... current shift = 0.000445520845658\n",
      "... current shift = 0.000896192262166\n",
      "... current shift = 0.000423511232513\n",
      "... current shift = 0.000792958741359\n",
      "... current shift = 0.000542594748382\n",
      "... current shift = 0.000687762180361\n",
      "... current shift = 0.000780057671401\n",
      "... current shift = 0.000212051581171\n",
      "... current shift = 0.00572045628016\n",
      "@epoch 2\n",
      "   ELBO = -4447472.03846\n",
      "   Perplexity = 473072.009268\n",
      "(time elapsed: 798.888237)\n",
      "Epoch: 3\n",
      "... current shift = 0.00974466854595\n",
      "... current shift = 0.000567497705097\n",
      "... current shift = 0.000356965756417\n",
      "... current shift = 0.000454321644613\n",
      "... current shift = 0.000442034179926\n",
      "... current shift = 0.000664837909252\n",
      "... current shift = 0.00011218689941\n",
      "... current shift = 0.00773658458171\n",
      "... current shift = 0.00125194046288\n",
      "... current shift = 0.00476926213813\n",
      "@epoch 3\n",
      "   ELBO = -4495330.94396\n",
      "   Perplexity = 544496.019691\n",
      "(time elapsed: 842.051971197)\n",
      "Epoch: 4\n",
      "... current shift = 0.000952651431775\n",
      "... current shift = 0.000267700886773\n",
      "... current shift = 0.000912679353086\n",
      "... current shift = 0.000173245217621\n",
      "... current shift = 0.000311361138735\n",
      "... current shift = 0.00032709327098\n",
      "... current shift = 0.000503778184132\n",
      "... current shift = 0.816828395013\n",
      "... current shift = 0.848467773689\n",
      "... current shift = 0.0696077982574\n",
      "@epoch 4\n",
      "   ELBO = -4429023.98799\n",
      "   Perplexity = 448113.19821\n",
      "(time elapsed: 887.41568017)\n",
      "Epoch: 5\n",
      "... current shift = 0.000486839970604\n",
      "... current shift = 0.000984829013077\n",
      "... current shift = 0.000497163874262\n",
      "... current shift = 0.000832077190235\n",
      "... current shift = 0.0432706341642\n",
      "... current shift = 0.000497667937295\n",
      "... current shift = 0.000630401298545\n",
      "... current shift = 0.000394347316507\n",
      "... current shift = 0.000334712071325\n",
      "... current shift = 0.000493683122124\n",
      "@epoch 5\n",
      "   ELBO = -4384280.25431\n",
      "   Perplexity = 392911.920191\n",
      "(time elapsed: 933.589715004)\n",
      "CPU times: user 3min 50s, sys: 7 ms, total: 3min 50s\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Alpha = np.ones(K); Alpha.fill(alpha)\n",
    "Eta   = np.ones((K, W)); Eta.fill(eta)\n",
    "\n",
    "term4 = gammaln(K*alpha) - K*gammaln(alpha) + (gammaln(W*eta) - W*gammaln(eta))/num_docs\n",
    "\n",
    "Lambda = np.random.gamma(100., 1./100, (K, W))\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    print 'Epoch:', e+1\n",
    "    L = 0\n",
    "\n",
    "    random.shuffle(doc_ids)\n",
    "    for d in doc_ids:\n",
    "\n",
    "        n_d = N[d]\n",
    "\n",
    "        shift = np.inf\n",
    "        epsilon = 1e-3\n",
    "\n",
    "        Gamma_d = np.random.gamma(100., 1./100, K)                  # K\n",
    "        for _ in range(num_iters):\n",
    "\n",
    "            sum_Gamma_d = Gamma_d.sum()                             # 1\n",
    "            sum_Lambda  = Lambda.sum(axis=1)                        # K x 1\n",
    "\n",
    "            Eq_logtheta_d = digamma(Gamma_d) - digamma(sum_Gamma_d)               # K - 1 = K\n",
    "            Eq_logbeta    = digamma(Lambda) - digamma(sum_Lambda[:,np.newaxis])   # K x W - K x 1 = K x W\n",
    "            \n",
    "            Eq_logtheta_add_Eq_logbeta = Eq_logtheta_d[:,np.newaxis] + Eq_logbeta # for likelihood computation later.\n",
    "\n",
    "            Phi_d = np.exp(Eq_logtheta_add_Eq_logbeta)  # K x W\n",
    "            Phi_d = normalize(Phi_d, norm='l1', axis=0) # each word relates to a mult. over topics\n",
    "            n_Phi_d = Phi_d * n_d # K x W\n",
    "\n",
    "            Gamma_delta = Alpha + n_Phi_d.sum(axis=1) # K\n",
    "\n",
    "            shift = (1/K)*abs(Gamma_d - Gamma_delta).sum()\n",
    "            if shift < epsilon:\n",
    "                break\n",
    "\n",
    "            Gamma_d = Gamma_delta\n",
    "\n",
    "        if d%50==0:\n",
    "            print '... current shift =', shift\n",
    "\n",
    "        Lambda_hat = Eta + num_docs*n_Phi_d # K x W\n",
    "        Lambda = (1 - rho(d))*Lambda + rho(d)*Lambda_hat\n",
    "        \n",
    "        # COMPUTE LIKELIHOOD (at some point)\n",
    "        term1 = (n_d * (Phi_d * (Eq_logtheta_add_Eq_logbeta - np.log(Phi_d))).sum(axis=0)).sum()\n",
    "        term2 = -gammaln(sum_Gamma_d) + ((Alpha-Gamma_d)*Eq_logtheta_d + gammaln(Gamma_d)).sum()\n",
    "        term3 = (-gammaln(sum_Lambda) + ((Eta-Lambda)*Eq_logbeta + gammaln(Lambda)).sum(axis=1)).sum() / num_docs \n",
    "        l = term1 + term2 + term3 + term4\n",
    "        L += l\n",
    "    \n",
    "    print \"@epoch\", e+1\n",
    "    print \"   ELBO =\", L\n",
    "    print \"   Perplexity =\", np.exp(-(L/num_tokens))    \n",
    "\n",
    "    print '(time elapsed:', str(time.time()-t)+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_words(topic, topk=20):\n",
    "    word_dist = Lambda[topic] / Lambda[topic].sum()\n",
    "    print map(lambda i:i2w[i], np.argsort(word_dist)[::-1][:topk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "[u'state', u'candidate', u'committee', u'democratic', u'issue', u'court', u'administration', u'aid', u'announced', u'nomination', u'president', u'department', u'federal', u'political', u'governor', u'chairman', u'senate', u'legislation', u'plans', u'congress']\n",
      "Topic: 1\n",
      "[u'primary', u'trial', u'think', u'open', u'help', u'land', u'going', u'care', u'april', u'dead', u'safety', u'red', u'right', u'showed', u'ahead', u'muscle', u'small', u'proud', u'knew', u'later']\n",
      "Topic: 2\n",
      "[u'number', u'medium', u'areas', u'research', u'cells', u'conservation', u'normal', u'major', u'health', u'provide', u'degree', u'available', u'activity', u'obtained', u'costs', u'plastics', u'increased', u'use', u'increase', u'results']\n",
      "Topic: 3\n",
      "[u'home', u'night', u'asked', u'school', u'house', u'hospital', u'day', u'old', u'days', u'sheets', u'held', u'told', u'death', u'mother', u'took', u'long', u'office', u'ask', u'place', u'sign']\n",
      "Topic: 4\n",
      "[u'president', u'tax', u'government', u'medical', u'statements', u'statement', u'cases', u'service', u'national', u'today', u'signs', u'attack', u'year', u'states', u'case', u'military', u'persons', u'united', u'involved', u'grants']\n",
      "Topic: 5\n",
      "[u'industry', u'jury', u'candidates', u'bridge', u'neighbor', u'large', u'business', u'section', u'cross', u'walker', u'area', u'charge', u'reports', u'purchase', u'help', u'added', u'machines', u'tractor', u'planned', u'introduced']\n",
      "Topic: 6\n",
      "[u'county', u'campaign', u'jersey', u'remark', u'radio', u'developed', u'counties', u'remains', u'life', u'sensitive', u'reading', u'receive', u'original', u'material', u'complete', u'experience', u'readings', u'techniques', u'grand', u'known']\n",
      "Topic: 7\n",
      "[u'people', u'yesterday', u'today', u'come', u'children', u'left', u'given', u'women', u'club', u'evidence', u'students', u'meet', u'called', u'little', u'arms', u'patient', u'weakness', u'early', u'taken', u'past']\n",
      "Topic: 8\n",
      "[u'plan', u'year', u'election', u'million', u'program', u'issues', u'public', u'members', u'pay', u'schools', u'law', u'vote', u'services', u'funds', u'recent', u'proposed', u'general', u'education', u'elected', u'voting']\n",
      "Topic: 9\n",
      "[u'party', u'years', u'judge', u'contained', u'cent', u'william', u'week', u'burns', u'john', u'meeting', u'received', u'retired', u'taxes', u'test', u'expected', u'seeking', u'city', u'charged', u'offer', u'force']\n"
     ]
    }
   ],
   "source": [
    "for topic in range(K):\n",
    "    print 'Topic:', topic\n",
    "    top_words(topic, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Variational Bayes (Mini-Batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = {}\n",
    "num_tokens = 0\n",
    "for d,doc in enumerate(brown_docs):\n",
    "    w2c = Counter(doc)\n",
    "    N[d] = np.array([w2c[w] for w in vocab])\n",
    "    num_tokens += N[d].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 10 # number of topics\n",
    "W = len(vocab)\n",
    "\n",
    "alpha = 0.01\n",
    "eta   = 0.01\n",
    "tau0  = 1.0\n",
    "kappa = 0.5\n",
    "rho   = lambda t: np.power(tau0 + t, -kappa)\n",
    "\n",
    "num_docs   = len(N)\n",
    "num_epochs = 5 # Doesn't have too. If too many docs, 1 pass.\n",
    "num_iters  = 10\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "doc_ids = range(num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "... current shift = 0.918569088914\n",
      "... current shift = 2.09215378584\n",
      "... current shift = 2.28966296829\n",
      "... current shift = 3.25613512494\n",
      "... current shift = 1.29245904396\n",
      "... current shift = 0.863406840169\n",
      "... current shift = 1.12765526966\n",
      "... current shift = 1.80294429832\n",
      "... current shift = 0.139416608273\n",
      "... current shift = 0.377625763314\n",
      "@epoch 1\n",
      "   ELBO = -28516065.3403\n",
      "   Perplexity = 2.43331079158e+36\n",
      "(time elapsed: 1047.57516408)\n",
      "Epoch: 2\n",
      "... current shift = 0.475328863889\n",
      "... current shift = 0.293460987258\n",
      "... current shift = 0.288433391785\n",
      "... current shift = 0.592354977604\n",
      "... current shift = 0.0729993856156\n",
      "... current shift = 0.340496535977\n",
      "... current shift = 0.176994585004\n",
      "... current shift = 0.0733331505887\n",
      "... current shift = 0.194059968369\n",
      "... current shift = 0.106563928432\n",
      "@epoch 2\n",
      "   ELBO = -25963585.4265\n",
      "   Perplexity = 1.34666702412e+33\n",
      "(time elapsed: 1096.67845511)\n",
      "Epoch: 3\n",
      "... current shift = 0.153179627433\n",
      "... current shift = 0.364054970768\n",
      "... current shift = 0.493724443687\n",
      "... current shift = 0.279267062756\n",
      "... current shift = 0.118360911142\n",
      "... current shift = 0.238839407456\n",
      "... current shift = 0.173968082897\n",
      "... current shift = 0.268525841105\n",
      "... current shift = 0.126115213604\n",
      "... current shift = 0.226923834413\n",
      "@epoch 3\n",
      "   ELBO = -25506503.7331\n",
      "   Perplexity = 3.51583772873e+32\n",
      "(time elapsed: 1145.99743009)\n",
      "Epoch: 4\n",
      "... current shift = 0.179746482409\n",
      "... current shift = 0.0574419360706\n",
      "... current shift = 0.0711695093202\n",
      "... current shift = 0.0960031342284\n",
      "... current shift = 0.0879790919139\n",
      "... current shift = 0.208137099478\n",
      "... current shift = 0.107260774697\n",
      "... current shift = 0.105748520961\n",
      "... current shift = 0.0490994899175\n",
      "... current shift = 0.0780449386346\n",
      "@epoch 4\n",
      "   ELBO = -25402059.4856\n",
      "   Perplexity = 2.586777459e+32\n",
      "(time elapsed: 1195.46355009)\n",
      "Epoch: 5\n",
      "... current shift = 0.0587713165317\n",
      "... current shift = 0.11174124655\n",
      "... current shift = 0.0292825171109\n",
      "... current shift = 0.0840669566136\n",
      "... current shift = 0.0339965777373\n",
      "... current shift = 0.100648132252\n",
      "... current shift = 0.0251105722058\n",
      "... current shift = 0.0345186808047\n",
      "... current shift = 0.123362110904\n",
      "... current shift = 0.113758696836\n",
      "@epoch 5\n",
      "   ELBO = -25400179.1653\n",
      "   Perplexity = 2.57252615542e+32\n",
      "(time elapsed: 1244.7651422)\n",
      "CPU times: user 4min 6s, sys: 7 ms, total: 4min 6s\n",
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "Alpha = np.ones(K); Alpha.fill(alpha)\n",
    "Eta   = np.ones((K, W)); Eta.fill(eta)\n",
    "\n",
    "term4 = gammaln(K*alpha) - K*gammaln(alpha) + (gammaln(W*eta) - W*gammaln(eta))/num_docs\n",
    "\n",
    "Lambda = np.random.gamma(100., 1./100, (K, W))\n",
    "\n",
    "doc_count = 0\n",
    "n_Phi = np.zeros((K, W))\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    \n",
    "    print 'Epoch:', e+1\n",
    "    L = 0\n",
    "    \n",
    "    random.shuffle(doc_ids)\n",
    "    for d in doc_ids:\n",
    "        \n",
    "        doc_count += 1\n",
    "\n",
    "        n_d = N[d]\n",
    "\n",
    "        shift = np.inf\n",
    "        epsilon = 1e-3\n",
    "\n",
    "        Gamma_d = np.random.gamma(100., 1./100, K)                  # K\n",
    "        for _ in range(num_iters):\n",
    "\n",
    "            sum_Gamma_d = Gamma_d.sum()                             # 1\n",
    "            sum_Lambda  = Lambda.sum(axis=1)[:,np.newaxis]          # K x 1\n",
    "\n",
    "            Eq_logtheta_d = digamma(Gamma_d) - digamma(sum_Gamma_d) # K - 1 = K\n",
    "            Eq_logbeta    = digamma(Lambda) - digamma(sum_Lambda)   # K x W - K x 1 = K x W\n",
    "\n",
    "            Phi_d = np.exp(Eq_logtheta_d[:,np.newaxis] + Eq_logbeta) # K x W\n",
    "            Phi_d = normalize(Phi_d, norm='l1', axis=0) # each word relates to a mult. over topics\n",
    "            n_Phi_d = Phi_d * n_d # K x W\n",
    "\n",
    "            Gamma_delta = Alpha + n_Phi_d.sum(axis=1) # K\n",
    "\n",
    "            shift = (1/K)*abs(Gamma_d - Gamma_delta).sum()\n",
    "            if shift < epsilon:\n",
    "                break\n",
    "\n",
    "            Gamma_d = Gamma_delta\n",
    "            \n",
    "        if d%50==0:\n",
    "            print '... current shift =', shift\n",
    "\n",
    "        n_Phi += n_Phi_d\n",
    "        \n",
    "        # COMPUTE LIKELIHOOD (at some point)\n",
    "        term1 = (n_d * (Phi_d * (Eq_logtheta_add_Eq_logbeta - np.log(Phi_d))).sum(axis=0)).sum()\n",
    "        term2 = -gammaln(sum_Gamma_d) + ((Alpha-Gamma_d)*Eq_logtheta_d + gammaln(Gamma_d)).sum()\n",
    "        term3 = (-gammaln(sum_Lambda) + ((Eta-Lambda)*Eq_logbeta + gammaln(Lambda)).sum(axis=1)).sum() / num_docs \n",
    "        l = term1 + term2 + term3 + term4\n",
    "        L += l        \n",
    "        \n",
    "        if doc_count%batch_size==0:      \n",
    "            Lambda_hat = Eta + (num_docs/batch_size)*n_Phi # K x W\n",
    "            Lambda = (1 - rho(d))*Lambda + rho(d)*Lambda_hat\n",
    "            n_Phi = np.zeros((K, W))\n",
    "\n",
    "    print \"@epoch\", e+1\n",
    "    print \"   ELBO =\", L\n",
    "    print \"   Perplexity =\", np.exp(-(L/num_tokens))             \n",
    "            \n",
    "    print '(time elapsed:', str(time.time()-t)+')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_words(topic, topk=20):\n",
    "    word_dist = Lambda[topic] / Lambda[topic].sum()\n",
    "    print map(lambda i:i2w[i], np.argsort(word_dist)[::-1][:topk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "[u'state', u'states', u'united', u'government', u'community', u'local', u'development', u'program', u'public', u'business', u'year', u'department', u'general', u'national', u'areas', u'act', u'service', u'cent', u'society', u'fiscal']\n",
      "Topic: 1\n",
      "[u'years', u'year', u'day', u'city', u'john', u'home', u'week', u'school', u'good', u'house', u'men', u'days', u'board', u'high', u'president', u'radio', u'farm', u'great', u'north', u'war']\n",
      "Topic: 2\n",
      "[u'music', u'labor', u'radiation', u'moon', u'level', u'opportunity', u'earth', u'stock', u'programs', u'condition', u'estate', u'individual', u'observations', u'systems', u'stations', u'appeal', u'ship', u'station', u'decision', u'marine']\n",
      "Topic: 3\n",
      "[u'values', u'number', u'class', u'order', u'point', u'different', u'analysis', u'change', u'theory', u'value', u'image', u'lines', u'larger', u'line', u'particular', u'skywave', u'obvious', u'general', u'paper', u'space']\n",
      "Topic: 4\n",
      "[u'world', u'life', u'people', u'great', u'fact', u'way', u'sense', u'experience', u'human', u'work', u'course', u'men', u'kind', u'power', u'form', u'church', u'far', u'art', u'history', u'example']\n",
      "Topic: 5\n",
      "[u'social', u'economic', u'nations', u'students', u'poems', u'growth', u'defense', u'process', u'student', u'union', u'needed', u'poetry', u'income', u'leaders', u'religious', u'organization', u'groups', u'ideas', u'member', u'schools']\n",
      "Topic: 6\n",
      "[u'love', u'minutes', u'river', u'captain', u'miles', u'lost', u'return', u'cars', u'outside', u'months', u'beautiful', u'returned', u'friend', u'innocent', u'hardy', u'camp', u'remained', u'cotton', u'wrong', u'watch']\n",
      "Topic: 7\n",
      "[u'little', u'know', u'came', u'went', u'room', u'got', u'knew', u'right', u'head', u'thought', u'long', u'way', u'come', u'looked', u'saw', u'eyes', u'away', u'old', u'look', u'left']\n",
      "Topic: 8\n",
      "[u'emission', u'temperature', u'small', u'pressure', u'volume', u'information', u'observed', u'intensity', u'material', u'surface', u'thermal', u'data', u'reaction', u'low', u'sections', u'high', u'variation', u'range', u'normal', u'measurements']\n",
      "Topic: 9\n",
      "[u'water', u'feed', u'use', u'foods', u'stock', u'soil', u'cut', u'pansies', u'machine', u'disk', u'surface', u'salt', u'antenna', u'temperature', u'heat', u'chemical', u'avocado', u'seeds', u'affixed', u'processing']\n"
     ]
    }
   ],
   "source": [
    "for topic in range(K):\n",
    "    print 'Topic:', topic\n",
    "    top_words(topic, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
